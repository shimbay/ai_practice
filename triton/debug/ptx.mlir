//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_89
.address_size 64

	// .globl	matmul_split_k_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_split_k_kernel(
	.param .u64 matmul_split_k_kernel_param_0,
	.param .u64 matmul_split_k_kernel_param_1,
	.param .u64 matmul_split_k_kernel_param_2,
	.param .u32 matmul_split_k_kernel_param_3,
	.param .u32 matmul_split_k_kernel_param_4,
	.param .u32 matmul_split_k_kernel_param_5,
	.param .u32 matmul_split_k_kernel_param_6,
	.param .u32 matmul_split_k_kernel_param_7,
	.param .u32 matmul_split_k_kernel_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<110>;
	.reg .b32 	%r<509>;
	.reg .f32 	%f<289>;
	.reg .b64 	%rd<106>;
	.loc	1 7 0
$L__func_begin0:
	.loc	1 7 0

	ld.param.u64 	%rd41, [matmul_split_k_kernel_param_0];
	ld.param.u64 	%rd42, [matmul_split_k_kernel_param_1];
$L__tmp0:
	.loc	1 25 26
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd43, [matmul_split_k_kernel_param_2];
	.loc	1 26 26
	// begin inline asm
	mov.u32 %r2, %ctaid.y;
	// end inline asm
	ld.param.u32 	%r364, [matmul_split_k_kernel_param_3];
	.loc	1 27 26
	// begin inline asm
	mov.u32 %r3, %ctaid.z;
	// end inline asm
	.loc	1 32 20
	shl.b32 	%r365, %r2, 6;
	ld.param.u32 	%r366, [matmul_split_k_kernel_param_4];
	ld.param.u32 	%r367, [matmul_split_k_kernel_param_5];
	ld.param.u32 	%r368, [matmul_split_k_kernel_param_6];
	.loc	1 41 8
	cvt.s64.s32 	%rd44, %r368;
	ld.param.u32 	%r369, [matmul_split_k_kernel_param_7];
	.loc	1 49 8
	cvt.s64.s32 	%rd45, %r369;
	ld.param.u32 	%r370, [matmul_split_k_kernel_param_8];
	.loc	1 53 16
	mov.u32 	%r371, %tid.x;
	shr.u32 	%r372, %r371, 4;
	shl.b32 	%r373, %r371, 2;
	and.b32  	%r374, %r373, 4;
	.loc	1 31 20
	cvt.u64.u32 	%rd46, %r3;
	shl.b64 	%rd47, %rd46, 37;
	shl.b32 	%r375, %r3, 5;
	shl.b32 	%r376, %r1, 6;
	.loc	1 53 16
	and.b32  	%r377, %r373, 28;
	bfe.u32 	%r378, %r371, 3, 4;
	or.b32  	%r379, %r378, 16;
	or.b32  	%r380, %r378, 32;
	or.b32  	%r381, %r378, 48;
	cvt.u64.u32 	%rd48, %r379;
	cvt.u64.u32 	%rd49, %r380;
	cvt.u64.u32 	%rd50, %r381;
	cvt.s64.s32 	%rd51, %r376;
	or.b64  	%rd52, %rd51, %rd48;
	or.b64  	%rd53, %rd51, %rd49;
	or.b64  	%rd54, %rd51, %rd50;
	or.b32  	%r382, %r375, %r377;
	or.b32  	%r383, %r376, %r378;
	mul.wide.s32 	%rd55, %r383, %r368;
	mul.lo.s64 	%rd56, %rd52, %rd44;
	mul.lo.s64 	%rd57, %rd53, %rd44;
	mul.lo.s64 	%rd58, %rd54, %rd44;
	shl.b64 	%rd59, %rd55, 2;
	add.s64 	%rd60, %rd41, %rd59;
	mul.wide.s32 	%rd61, %r382, 4;
	add.s64 	%rd1, %rd60, %rd61;
	shl.b64 	%rd62, %rd56, 2;
	add.s64 	%rd63, %rd41, %rd62;
	add.s64 	%rd2, %rd63, %rd61;
	shl.b64 	%rd64, %rd57, 2;
	add.s64 	%rd65, %rd41, %rd64;
	add.s64 	%rd3, %rd65, %rd61;
	shl.b64 	%rd66, %rd58, 2;
	add.s64 	%rd67, %rd41, %rd66;
	add.s64 	%rd4, %rd67, %rd61;
	setp.gt.s64 	%p73, %rd52, -1;
	setp.gt.s64 	%p74, %rd53, -1;
	setp.gt.s64 	%p75, %rd54, -1;
	cvt.s64.s32 	%rd68, %r364;
	setp.lt.s64 	%p76, %rd52, %rd68;
	setp.lt.s64 	%p77, %rd53, %rd68;
	setp.lt.s64 	%p78, %rd54, %rd68;
	and.pred  	%p79, %p73, %p76;
	and.pred  	%p80, %p74, %p77;
	and.pred  	%p81, %p75, %p78;
	setp.gt.s32 	%p82, %r375, -1;
	setp.gt.s32 	%p83, %r376, -1;
	setp.lt.s32 	%p84, %r382, %r367;
	setp.lt.s32 	%p85, %r383, %r364;
	and.pred  	%p86, %p83, %p85;
	and.pred  	%p87, %p82, %p84;
	and.pred  	%p1, %p86, %p87;
	and.pred  	%p6, %p79, %p87;
	and.pred  	%p11, %p80, %p87;
	and.pred  	%p16, %p81, %p87;
	mov.b32 	%r8, 0;
	// begin inline asm
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	mov.u32 %r6, 0x0;
	mov.u32 %r7, 0x0;
	@%p1 ld.global.v4.b32 { %r4, %r5, %r6, %r7 }, [ %rd1 + 0 ];
	@!%p1 mov.u32 %r4, %r8;
	@!%p1 mov.u32 %r5, %r8;
	@!%p1 mov.u32 %r6, %r8;
	@!%p1 mov.u32 %r7, %r8;
	// end inline asm
	// begin inline asm
	mov.u32 %r12, 0x0;
	mov.u32 %r13, 0x0;
	mov.u32 %r14, 0x0;
	mov.u32 %r15, 0x0;
	@%p6 ld.global.v4.b32 { %r12, %r13, %r14, %r15 }, [ %rd2 + 0 ];
	@!%p6 mov.u32 %r12, %r8;
	@!%p6 mov.u32 %r13, %r8;
	@!%p6 mov.u32 %r14, %r8;
	@!%p6 mov.u32 %r15, %r8;
	// end inline asm
	// begin inline asm
	mov.u32 %r20, 0x0;
	mov.u32 %r21, 0x0;
	mov.u32 %r22, 0x0;
	mov.u32 %r23, 0x0;
	@%p11 ld.global.v4.b32 { %r20, %r21, %r22, %r23 }, [ %rd3 + 0 ];
	@!%p11 mov.u32 %r20, %r8;
	@!%p11 mov.u32 %r21, %r8;
	@!%p11 mov.u32 %r22, %r8;
	@!%p11 mov.u32 %r23, %r8;
	// end inline asm
	// begin inline asm
	mov.u32 %r28, 0x0;
	mov.u32 %r29, 0x0;
	mov.u32 %r30, 0x0;
	mov.u32 %r31, 0x0;
	@%p16 ld.global.v4.b32 { %r28, %r29, %r30, %r31 }, [ %rd4 + 0 ];
	@!%p16 mov.u32 %r28, %r8;
	@!%p16 mov.u32 %r29, %r8;
	@!%p16 mov.u32 %r30, %r8;
	@!%p16 mov.u32 %r31, %r8;
	// end inline asm
	shr.u32 	%r384, %r371, 1;
	xor.b32  	%r385, %r373, %r384;
	and.b32  	%r386, %r385, 28;
	shl.b32 	%r387, %r378, 7;
	shl.b32 	%r388, %r386, 2;
	or.b32  	%r389, %r387, %r388;
	mov.u32 	%r390, global_smem;
	add.s32 	%r391, %r390, %r389;
	shl.b32 	%r392, %r379, 7;
	or.b32  	%r393, %r392, %r388;
	add.s32 	%r394, %r390, %r393;
	shl.b32 	%r395, %r380, 7;
	or.b32  	%r396, %r395, %r388;
	add.s32 	%r397, %r390, %r396;
	shl.b32 	%r398, %r381, 7;
	or.b32  	%r399, %r398, %r388;
	add.s32 	%r400, %r390, %r399;
	st.shared.v4.u32 	[%r391], {%r4, %r5, %r6, %r7};
	st.shared.v4.u32 	[%r394], {%r12, %r13, %r14, %r15};
	st.shared.v4.u32 	[%r397], {%r20, %r21, %r22, %r23};
	st.shared.v4.u32 	[%r400], {%r28, %r29, %r30, %r31};
	bfe.u32 	%r401, %r371, 4, 3;
	or.b32  	%r402, %r401, 8;
	or.b32  	%r403, %r401, 16;
	or.b32  	%r404, %r401, 24;
	cvt.u64.u32 	%rd69, %r402;
	cvt.u64.u32 	%rd70, %r403;
	cvt.u64.u32 	%rd71, %r404;
	shr.s64 	%rd72, %rd47, 32;
	or.b64  	%rd73, %rd72, %rd69;
	or.b64  	%rd74, %rd72, %rd70;
	or.b64  	%rd75, %rd72, %rd71;
	.loc	1 54 16
	or.b32  	%r405, %r375, %r401;
	mul.wide.s32 	%rd76, %r405, %r369;
	mul.lo.s64 	%rd77, %rd73, %rd45;
	mul.lo.s64 	%rd78, %rd74, %rd45;
	mul.lo.s64 	%rd79, %rd75, %rd45;
	shl.b64 	%rd80, %rd76, 2;
	add.s64 	%rd81, %rd42, %rd80;
	.loc	1 53 16
	and.b32  	%r406, %r373, 60;
	.loc	1 54 16
	or.b32  	%r407, %r365, %r406;
	.loc	1 62 51
	mul.wide.s32 	%rd82, %r407, 4;
	.loc	1 54 16
	add.s64 	%rd5, %rd81, %rd82;
	shl.b64 	%rd83, %rd77, 2;
	add.s64 	%rd84, %rd42, %rd83;
	add.s64 	%rd6, %rd84, %rd82;
	shl.b64 	%rd85, %rd78, 2;
	add.s64 	%rd86, %rd42, %rd85;
	add.s64 	%rd7, %rd86, %rd82;
	shl.b64 	%rd87, %rd79, 2;
	add.s64 	%rd88, %rd42, %rd87;
	add.s64 	%rd8, %rd88, %rd82;
	setp.gt.s64 	%p88, %rd73, -1;
	setp.gt.s64 	%p89, %rd74, -1;
	setp.gt.s64 	%p90, %rd75, -1;
	cvt.s64.s32 	%rd89, %r367;
	setp.lt.s64 	%p91, %rd73, %rd89;
	setp.lt.s64 	%p92, %rd74, %rd89;
	setp.lt.s64 	%p93, %rd75, %rd89;
	and.pred  	%p94, %p88, %p91;
	and.pred  	%p95, %p89, %p92;
	and.pred  	%p96, %p90, %p93;
	setp.gt.s32 	%p97, %r365, -1;
	setp.lt.s32 	%p98, %r405, %r367;
	and.pred  	%p99, %p82, %p98;
	setp.lt.s32 	%p100, %r407, %r366;
	and.pred  	%p101, %p97, %p100;
	and.pred  	%p21, %p101, %p99;
	and.pred  	%p26, %p101, %p94;
	and.pred  	%p31, %p101, %p95;
	and.pred  	%p36, %p101, %p96;
	// begin inline asm
	mov.u32 %r36, 0x0;
	mov.u32 %r37, 0x0;
	mov.u32 %r38, 0x0;
	mov.u32 %r39, 0x0;
	@%p21 ld.global.v4.b32 { %r36, %r37, %r38, %r39 }, [ %rd5 + 0 ];
	@!%p21 mov.u32 %r36, %r8;
	@!%p21 mov.u32 %r37, %r8;
	@!%p21 mov.u32 %r38, %r8;
	@!%p21 mov.u32 %r39, %r8;
	// end inline asm
	// begin inline asm
	mov.u32 %r44, 0x0;
	mov.u32 %r45, 0x0;
	mov.u32 %r46, 0x0;
	mov.u32 %r47, 0x0;
	@%p26 ld.global.v4.b32 { %r44, %r45, %r46, %r47 }, [ %rd6 + 0 ];
	@!%p26 mov.u32 %r44, %r8;
	@!%p26 mov.u32 %r45, %r8;
	@!%p26 mov.u32 %r46, %r8;
	@!%p26 mov.u32 %r47, %r8;
	// end inline asm
	// begin inline asm
	mov.u32 %r52, 0x0;
	mov.u32 %r53, 0x0;
	mov.u32 %r54, 0x0;
	mov.u32 %r55, 0x0;
	@%p31 ld.global.v4.b32 { %r52, %r53, %r54, %r55 }, [ %rd7 + 0 ];
	@!%p31 mov.u32 %r52, %r8;
	@!%p31 mov.u32 %r53, %r8;
	@!%p31 mov.u32 %r54, %r8;
	@!%p31 mov.u32 %r55, %r8;
	// end inline asm
	// begin inline asm
	mov.u32 %r60, 0x0;
	mov.u32 %r61, 0x0;
	mov.u32 %r62, 0x0;
	mov.u32 %r63, 0x0;
	@%p36 ld.global.v4.b32 { %r60, %r61, %r62, %r63 }, [ %rd8 + 0 ];
	@!%p36 mov.u32 %r60, %r8;
	@!%p36 mov.u32 %r61, %r8;
	@!%p36 mov.u32 %r62, %r8;
	@!%p36 mov.u32 %r63, %r8;
	// end inline asm
	and.b32  	%r408, %r384, 24;
	and.b32  	%r409, %r373, 56;
	xor.b32  	%r410, %r409, %r408;
	or.b32  	%r411, %r410, %r374;
	shl.b32 	%r412, %r401, 8;
	shl.b32 	%r413, %r411, 2;
	or.b32  	%r414, %r412, %r413;
	add.s32 	%r415, %r390, 8192;
	add.s32 	%r416, %r415, %r414;
	shl.b32 	%r417, %r402, 8;
	or.b32  	%r418, %r417, %r413;
	add.s32 	%r419, %r415, %r418;
	shl.b32 	%r420, %r403, 8;
	or.b32  	%r421, %r420, %r413;
	add.s32 	%r422, %r415, %r421;
	shl.b32 	%r423, %r404, 8;
	or.b32  	%r424, %r423, %r413;
	add.s32 	%r425, %r415, %r424;
	st.shared.v4.u32 	[%r416], {%r36, %r37, %r38, %r39};
	st.shared.v4.u32 	[%r419], {%r44, %r45, %r46, %r47};
	st.shared.v4.u32 	[%r422], {%r52, %r53, %r54, %r55};
	st.shared.v4.u32 	[%r425], {%r60, %r61, %r62, %r63};
	.loc	1 53 16
	bar.sync 	0;
	and.b32  	%r426, %r371, 7;
	bfe.u32 	%r427, %r371, 4, 1;
	shr.u32 	%r428, %r371, 2;
	and.b32  	%r429, %r428, 16;
	and.b32  	%r430, %r371, 15;
	or.b32  	%r431, %r430, %r429;
	xor.b32  	%r432, %r427, %r426;
	shl.b32 	%r433, %r432, 4;
	shl.b32 	%r434, %r431, 7;
	or.b32  	%r435, %r434, %r433;
	add.s32 	%r72, %r390, %r435;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r108, %r109, %r110, %r111 }, [ %r72 + 0 ];
	// end inline asm
	or.b32  	%r436, %r427, 2;
	xor.b32  	%r437, %r436, %r426;
	shl.b32 	%r438, %r437, 4;
	or.b32  	%r439, %r438, %r434;
	add.s32 	%r77, %r390, %r439;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r156, %r157, %r158, %r159 }, [ %r77 + 0 ];
	// end inline asm
	or.b32  	%r440, %r427, 4;
	xor.b32  	%r441, %r440, %r426;
	shl.b32 	%r442, %r441, 4;
	or.b32  	%r443, %r442, %r434;
	add.s32 	%r82, %r390, %r443;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r204, %r205, %r206, %r207 }, [ %r82 + 0 ];
	// end inline asm
	or.b32  	%r444, %r427, 6;
	xor.b32  	%r445, %r444, %r426;
	shl.b32 	%r446, %r445, 4;
	or.b32  	%r447, %r446, %r434;
	add.s32 	%r87, %r390, %r447;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r252, %r253, %r254, %r255 }, [ %r87 + 0 ];
	// end inline asm
	add.s32 	%r92, %r72, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r132, %r133, %r134, %r135 }, [ %r92 + 0 ];
	// end inline asm
	add.s32 	%r97, %r77, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r180, %r181, %r182, %r183 }, [ %r97 + 0 ];
	// end inline asm
	add.s32 	%r102, %r82, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r228, %r229, %r230, %r231 }, [ %r102 + 0 ];
	// end inline asm
	add.s32 	%r107, %r87, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r276, %r277, %r278, %r279 }, [ %r107 + 0 ];
	// end inline asm
	.loc	1 54 16
	bfe.u32 	%r448, %r371, 5, 1;
	bfe.u32 	%r449, %r371, 2, 3;
	and.b32  	%r450, %r371, 3;
	xor.b32  	%r451, %r448, %r450;
	shl.b32 	%r452, %r451, 3;
	or.b32  	%r453, %r452, %r449;
	shl.b32 	%r454, %r450, 6;
	or.b32  	%r455, %r453, %r454;
	or.b32  	%r456, %r448, 2;
	xor.b32  	%r457, %r456, %r450;
	shl.b32 	%r458, %r457, 3;
	or.b32  	%r459, %r458, %r449;
	or.b32  	%r460, %r459, %r454;
	shl.b32 	%r461, %r455, 2;
	add.s32 	%r462, %r415, %r461;
	shl.b32 	%r463, %r460, 2;
	add.s32 	%r464, %r415, %r463;
	ld.shared.u32 	%r112, [%r462];
	ld.shared.u32 	%r118, [%r464];
	ld.shared.u32 	%r113, [%r462+1024];
	ld.shared.u32 	%r119, [%r464+1024];
	ld.shared.u32 	%r160, [%r462+2048];
	ld.shared.u32 	%r166, [%r464+2048];
	ld.shared.u32 	%r161, [%r462+3072];
	ld.shared.u32 	%r167, [%r464+3072];
	ld.shared.u32 	%r208, [%r462+4096];
	ld.shared.u32 	%r214, [%r464+4096];
	ld.shared.u32 	%r209, [%r462+5120];
	ld.shared.u32 	%r215, [%r464+5120];
	ld.shared.u32 	%r256, [%r462+6144];
	ld.shared.u32 	%r262, [%r464+6144];
	ld.shared.u32 	%r257, [%r462+7168];
	ld.shared.u32 	%r263, [%r464+7168];
	or.b32  	%r465, %r448, 4;
	xor.b32  	%r466, %r465, %r450;
	shl.b32 	%r467, %r466, 3;
	or.b32  	%r468, %r467, %r449;
	or.b32  	%r469, %r468, %r454;
	or.b32  	%r470, %r448, 6;
	xor.b32  	%r471, %r470, %r450;
	shl.b32 	%r472, %r471, 3;
	or.b32  	%r473, %r472, %r449;
	or.b32  	%r474, %r473, %r454;
	shl.b32 	%r475, %r469, 2;
	add.s32 	%r476, %r415, %r475;
	shl.b32 	%r477, %r474, 2;
	add.s32 	%r478, %r415, %r477;
	ld.shared.u32 	%r124, [%r476];
	ld.shared.u32 	%r130, [%r478];
	ld.shared.u32 	%r125, [%r476+1024];
	ld.shared.u32 	%r131, [%r478+1024];
	ld.shared.u32 	%r172, [%r476+2048];
	ld.shared.u32 	%r178, [%r478+2048];
	ld.shared.u32 	%r173, [%r476+3072];
	ld.shared.u32 	%r179, [%r478+3072];
	ld.shared.u32 	%r220, [%r476+4096];
	ld.shared.u32 	%r226, [%r478+4096];
	ld.shared.u32 	%r221, [%r476+5120];
	ld.shared.u32 	%r227, [%r478+5120];
	ld.shared.u32 	%r268, [%r476+6144];
	ld.shared.u32 	%r274, [%r478+6144];
	ld.shared.u32 	%r269, [%r476+7168];
	ld.shared.u32 	%r275, [%r478+7168];
	mov.f32 	%f124, 0f00000000;
	.loc	1 57 20
	mov.f32 	%f65, %f124;
	mov.f32 	%f66, %f124;
	mov.f32 	%f67, %f124;
	mov.f32 	%f68, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f65, %f66, %f67, %f68 }, { %r108, %r109, %r110, %r111 }, { %r112, %r113 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	mov.f32 	%f73, %f124;
	mov.f32 	%f74, %f124;
	mov.f32 	%f75, %f124;
	mov.f32 	%f76, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f73, %f74, %f75, %f76 }, { %r108, %r109, %r110, %r111 }, { %r118, %r119 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	mov.f32 	%f81, %f124;
	mov.f32 	%f82, %f124;
	mov.f32 	%f83, %f124;
	mov.f32 	%f84, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f81, %f82, %f83, %f84 }, { %r108, %r109, %r110, %r111 }, { %r124, %r125 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	mov.f32 	%f89, %f124;
	mov.f32 	%f90, %f124;
	mov.f32 	%f91, %f124;
	mov.f32 	%f92, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f89, %f90, %f91, %f92 }, { %r108, %r109, %r110, %r111 }, { %r130, %r131 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	mov.f32 	%f97, %f124;
	mov.f32 	%f98, %f124;
	mov.f32 	%f99, %f124;
	mov.f32 	%f100, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f97, %f98, %f99, %f100 }, { %r132, %r133, %r134, %r135 }, { %r112, %r113 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	mov.f32 	%f105, %f124;
	mov.f32 	%f106, %f124;
	mov.f32 	%f107, %f124;
	mov.f32 	%f108, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f105, %f106, %f107, %f108 }, { %r132, %r133, %r134, %r135 }, { %r118, %r119 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	mov.f32 	%f113, %f124;
	mov.f32 	%f114, %f124;
	mov.f32 	%f115, %f124;
	mov.f32 	%f116, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f113, %f114, %f115, %f116 }, { %r132, %r133, %r134, %r135 }, { %r124, %r125 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	mov.f32 	%f121, %f124;
	mov.f32 	%f122, %f124;
	mov.f32 	%f123, %f124;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f121, %f122, %f123, %f124 }, { %r132, %r133, %r134, %r135 }, { %r130, %r131 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f65, %f66, %f67, %f68 }, { %r156, %r157, %r158, %r159 }, { %r160, %r161 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f73, %f74, %f75, %f76 }, { %r156, %r157, %r158, %r159 }, { %r166, %r167 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f81, %f82, %f83, %f84 }, { %r156, %r157, %r158, %r159 }, { %r172, %r173 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f89, %f90, %f91, %f92 }, { %r156, %r157, %r158, %r159 }, { %r178, %r179 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f97, %f98, %f99, %f100 }, { %r180, %r181, %r182, %r183 }, { %r160, %r161 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f105, %f106, %f107, %f108 }, { %r180, %r181, %r182, %r183 }, { %r166, %r167 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f113, %f114, %f115, %f116 }, { %r180, %r181, %r182, %r183 }, { %r172, %r173 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f121, %f122, %f123, %f124 }, { %r180, %r181, %r182, %r183 }, { %r178, %r179 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f65, %f66, %f67, %f68 }, { %r204, %r205, %r206, %r207 }, { %r208, %r209 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f73, %f74, %f75, %f76 }, { %r204, %r205, %r206, %r207 }, { %r214, %r215 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f81, %f82, %f83, %f84 }, { %r204, %r205, %r206, %r207 }, { %r220, %r221 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f89, %f90, %f91, %f92 }, { %r204, %r205, %r206, %r207 }, { %r226, %r227 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f97, %f98, %f99, %f100 }, { %r228, %r229, %r230, %r231 }, { %r208, %r209 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f105, %f106, %f107, %f108 }, { %r228, %r229, %r230, %r231 }, { %r214, %r215 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f113, %f114, %f115, %f116 }, { %r228, %r229, %r230, %r231 }, { %r220, %r221 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f121, %f122, %f123, %f124 }, { %r228, %r229, %r230, %r231 }, { %r226, %r227 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f65, %f66, %f67, %f68 }, { %r252, %r253, %r254, %r255 }, { %r256, %r257 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f73, %f74, %f75, %f76 }, { %r252, %r253, %r254, %r255 }, { %r262, %r263 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f81, %f82, %f83, %f84 }, { %r252, %r253, %r254, %r255 }, { %r268, %r269 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f89, %f90, %f91, %f92 }, { %r252, %r253, %r254, %r255 }, { %r274, %r275 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f97, %f98, %f99, %f100 }, { %r276, %r277, %r278, %r279 }, { %r256, %r257 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f105, %f106, %f107, %f108 }, { %r276, %r277, %r278, %r279 }, { %r262, %r263 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f113, %f114, %f115, %f116 }, { %r276, %r277, %r278, %r279 }, { %r268, %r269 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 { %f121, %f122, %f123, %f124 }, { %r276, %r277, %r278, %r279 }, { %r274, %r275 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	.loc	1 60 21
	or.b32  	%r479, %r376, %r401;
	or.b32  	%r480, %r376, %r402;
	or.b32  	%r481, %r376, %r403;
	or.b32  	%r482, %r376, %r404;
	or.b32  	%r483, %r479, 32;
	or.b32  	%r484, %r479, 40;
	or.b32  	%r485, %r479, 48;
	or.b32  	%r486, %r479, 56;
	.loc	1 62 39
	mul.lo.s32 	%r487, %r479, %r370;
	mul.lo.s32 	%r488, %r480, %r370;
	mul.lo.s32 	%r489, %r481, %r370;
	mul.lo.s32 	%r490, %r482, %r370;
	shl.b32 	%r491, %r370, 5;
	add.s32 	%r492, %r487, %r491;
	shl.b32 	%r493, %r370, 3;
	add.s32 	%r494, %r492, %r493;
	add.s32 	%r495, %r494, %r493;
	add.s32 	%r496, %r495, %r493;
	.loc	1 62 21
	mul.wide.s32 	%rd90, %r487, 4;
	add.s64 	%rd91, %rd43, %rd90;
	mul.wide.s32 	%rd92, %r488, 4;
	add.s64 	%rd93, %rd43, %rd92;
	mul.wide.s32 	%rd94, %r489, 4;
	add.s64 	%rd95, %rd43, %rd94;
	mul.wide.s32 	%rd96, %r490, 4;
	add.s64 	%rd97, %rd43, %rd96;
	mul.wide.s32 	%rd98, %r492, 4;
	add.s64 	%rd99, %rd43, %rd98;
	mul.wide.s32 	%rd100, %r494, 4;
	add.s64 	%rd101, %rd43, %rd100;
	mul.wide.s32 	%rd102, %r495, 4;
	add.s64 	%rd103, %rd43, %rd102;
	mul.wide.s32 	%rd104, %r496, 4;
	add.s64 	%rd105, %rd43, %rd104;
	.loc	1 62 51
	add.s64 	%rd9, %rd91, %rd82;
	add.s64 	%rd10, %rd9, 4;
	add.s64 	%rd11, %rd9, 8;
	add.s64 	%rd12, %rd9, 12;
	add.s64 	%rd13, %rd93, %rd82;
	add.s64 	%rd14, %rd13, 4;
	add.s64 	%rd15, %rd13, 8;
	add.s64 	%rd16, %rd13, 12;
	add.s64 	%rd17, %rd95, %rd82;
	add.s64 	%rd18, %rd17, 4;
	add.s64 	%rd19, %rd17, 8;
	add.s64 	%rd20, %rd17, 12;
	add.s64 	%rd21, %rd97, %rd82;
	add.s64 	%rd22, %rd21, 4;
	add.s64 	%rd23, %rd21, 8;
	add.s64 	%rd24, %rd21, 12;
	add.s64 	%rd25, %rd99, %rd82;
	add.s64 	%rd26, %rd25, 4;
	add.s64 	%rd27, %rd25, 8;
	add.s64 	%rd28, %rd25, 12;
	add.s64 	%rd29, %rd101, %rd82;
	add.s64 	%rd30, %rd29, 4;
	add.s64 	%rd31, %rd29, 8;
	add.s64 	%rd32, %rd29, 12;
	add.s64 	%rd33, %rd103, %rd82;
	add.s64 	%rd34, %rd33, 4;
	add.s64 	%rd35, %rd33, 8;
	add.s64 	%rd36, %rd33, 12;
	add.s64 	%rd37, %rd105, %rd82;
	add.s64 	%rd38, %rd37, 4;
	add.s64 	%rd39, %rd37, 8;
	add.s64 	%rd40, %rd37, 12;
	.loc	1 63 30
	setp.lt.s32 	%p102, %r479, %r364;
	setp.lt.s32 	%p103, %r480, %r364;
	setp.lt.s32 	%p104, %r481, %r364;
	setp.lt.s32 	%p105, %r482, %r364;
	setp.lt.s32 	%p106, %r483, %r364;
	setp.lt.s32 	%p107, %r484, %r364;
	setp.lt.s32 	%p108, %r485, %r364;
	setp.lt.s32 	%p109, %r486, %r364;
	.loc	1 63 36
	and.pred  	%p41, %p102, %p100;
	and.pred  	%p45, %p103, %p100;
	and.pred  	%p49, %p104, %p100;
	and.pred  	%p53, %p105, %p100;
	and.pred  	%p57, %p106, %p100;
	and.pred  	%p61, %p107, %p100;
	and.pred  	%p65, %p108, %p100;
	and.pred  	%p69, %p109, %p100;
	.loc	1 66 26
	bar.sync 	0;
	shl.b32 	%r497, %r450, 1;
	or.b32  	%r498, %r449, %r429;
	shl.b32 	%r499, %r448, 3;
	or.b32  	%r500, %r499, %r497;
	mad.lo.s32 	%r501, %r498, 68, %r500;
	shl.b32 	%r502, %r501, 2;
	add.s32 	%r503, %r390, %r502;
	st.shared.v2.f32 	[%r503], {%f65, %f66};
	st.shared.v2.f32 	[%r503+2176], {%f67, %f68};
	st.shared.v2.f32 	[%r503+64], {%f73, %f74};
	st.shared.v2.f32 	[%r503+2240], {%f75, %f76};
	st.shared.v2.f32 	[%r503+128], {%f81, %f82};
	st.shared.v2.f32 	[%r503+2304], {%f83, %f84};
	st.shared.v2.f32 	[%r503+192], {%f89, %f90};
	st.shared.v2.f32 	[%r503+2368], {%f91, %f92};
	bar.sync 	0;
	and.b32  	%r504, %r372, 6;
	or.b32  	%r505, %r504, %r427;
	mad.lo.s32 	%r506, %r505, 68, %r406;
	shl.b32 	%r507, %r506, 2;
	add.s32 	%r508, %r390, %r507;
	ld.shared.v4.f32 	{%f257, %f258, %f259, %f260}, [%r508];
	ld.shared.v4.f32 	{%f261, %f262, %f263, %f264}, [%r508+2176];
	ld.shared.v4.f32 	{%f265, %f266, %f267, %f268}, [%r508+4352];
	ld.shared.v4.f32 	{%f269, %f270, %f271, %f272}, [%r508+6528];
	bar.sync 	0;
	st.shared.v2.f32 	[%r503], {%f97, %f98};
	st.shared.v2.f32 	[%r503+2176], {%f99, %f100};
	st.shared.v2.f32 	[%r503+64], {%f105, %f106};
	st.shared.v2.f32 	[%r503+2240], {%f107, %f108};
	st.shared.v2.f32 	[%r503+128], {%f113, %f114};
	st.shared.v2.f32 	[%r503+2304], {%f115, %f116};
	st.shared.v2.f32 	[%r503+192], {%f121, %f122};
	st.shared.v2.f32 	[%r503+2368], {%f123, %f124};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f273, %f274, %f275, %f276}, [%r508];
	ld.shared.v4.f32 	{%f277, %f278, %f279, %f280}, [%r508+2176];
	ld.shared.v4.f32 	{%f281, %f282, %f283, %f284}, [%r508+4352];
	ld.shared.v4.f32 	{%f285, %f286, %f287, %f288}, [%r508+6528];
	mov.b32 	%r301, %f257;
	// begin inline asm
	mov.u32 %r300, 0x0;
	@%p41 atom.global.gpu.acq_rel.add.f32 %r300, [ %rd9 + 0 ], %r301;
	// end inline asm
	mov.b32 	%r303, %f258;
	// begin inline asm
	mov.u32 %r302, 0x0;
	@%p41 atom.global.gpu.acq_rel.add.f32 %r302, [ %rd10 + 0 ], %r303;
	// end inline asm
	mov.b32 	%r305, %f259;
	// begin inline asm
	mov.u32 %r304, 0x0;
	@%p41 atom.global.gpu.acq_rel.add.f32 %r304, [ %rd11 + 0 ], %r305;
	// end inline asm
	mov.b32 	%r307, %f260;
	// begin inline asm
	mov.u32 %r306, 0x0;
	@%p41 atom.global.gpu.acq_rel.add.f32 %r306, [ %rd12 + 0 ], %r307;
	// end inline asm
	mov.b32 	%r309, %f261;
	// begin inline asm
	mov.u32 %r308, 0x0;
	@%p45 atom.global.gpu.acq_rel.add.f32 %r308, [ %rd13 + 0 ], %r309;
	// end inline asm
	mov.b32 	%r311, %f262;
	// begin inline asm
	mov.u32 %r310, 0x0;
	@%p45 atom.global.gpu.acq_rel.add.f32 %r310, [ %rd14 + 0 ], %r311;
	// end inline asm
	mov.b32 	%r313, %f263;
	// begin inline asm
	mov.u32 %r312, 0x0;
	@%p45 atom.global.gpu.acq_rel.add.f32 %r312, [ %rd15 + 0 ], %r313;
	// end inline asm
	mov.b32 	%r315, %f264;
	// begin inline asm
	mov.u32 %r314, 0x0;
	@%p45 atom.global.gpu.acq_rel.add.f32 %r314, [ %rd16 + 0 ], %r315;
	// end inline asm
	mov.b32 	%r317, %f265;
	// begin inline asm
	mov.u32 %r316, 0x0;
	@%p49 atom.global.gpu.acq_rel.add.f32 %r316, [ %rd17 + 0 ], %r317;
	// end inline asm
	mov.b32 	%r319, %f266;
	// begin inline asm
	mov.u32 %r318, 0x0;
	@%p49 atom.global.gpu.acq_rel.add.f32 %r318, [ %rd18 + 0 ], %r319;
	// end inline asm
	mov.b32 	%r321, %f267;
	// begin inline asm
	mov.u32 %r320, 0x0;
	@%p49 atom.global.gpu.acq_rel.add.f32 %r320, [ %rd19 + 0 ], %r321;
	// end inline asm
	mov.b32 	%r323, %f268;
	// begin inline asm
	mov.u32 %r322, 0x0;
	@%p49 atom.global.gpu.acq_rel.add.f32 %r322, [ %rd20 + 0 ], %r323;
	// end inline asm
	mov.b32 	%r325, %f269;
	// begin inline asm
	mov.u32 %r324, 0x0;
	@%p53 atom.global.gpu.acq_rel.add.f32 %r324, [ %rd21 + 0 ], %r325;
	// end inline asm
	mov.b32 	%r327, %f270;
	// begin inline asm
	mov.u32 %r326, 0x0;
	@%p53 atom.global.gpu.acq_rel.add.f32 %r326, [ %rd22 + 0 ], %r327;
	// end inline asm
	mov.b32 	%r329, %f271;
	// begin inline asm
	mov.u32 %r328, 0x0;
	@%p53 atom.global.gpu.acq_rel.add.f32 %r328, [ %rd23 + 0 ], %r329;
	// end inline asm
	mov.b32 	%r331, %f272;
	// begin inline asm
	mov.u32 %r330, 0x0;
	@%p53 atom.global.gpu.acq_rel.add.f32 %r330, [ %rd24 + 0 ], %r331;
	// end inline asm
	mov.b32 	%r333, %f273;
	// begin inline asm
	mov.u32 %r332, 0x0;
	@%p57 atom.global.gpu.acq_rel.add.f32 %r332, [ %rd25 + 0 ], %r333;
	// end inline asm
	mov.b32 	%r335, %f274;
	// begin inline asm
	mov.u32 %r334, 0x0;
	@%p57 atom.global.gpu.acq_rel.add.f32 %r334, [ %rd26 + 0 ], %r335;
	// end inline asm
	mov.b32 	%r337, %f275;
	// begin inline asm
	mov.u32 %r336, 0x0;
	@%p57 atom.global.gpu.acq_rel.add.f32 %r336, [ %rd27 + 0 ], %r337;
	// end inline asm
	mov.b32 	%r339, %f276;
	// begin inline asm
	mov.u32 %r338, 0x0;
	@%p57 atom.global.gpu.acq_rel.add.f32 %r338, [ %rd28 + 0 ], %r339;
	// end inline asm
	mov.b32 	%r341, %f277;
	// begin inline asm
	mov.u32 %r340, 0x0;
	@%p61 atom.global.gpu.acq_rel.add.f32 %r340, [ %rd29 + 0 ], %r341;
	// end inline asm
	mov.b32 	%r343, %f278;
	// begin inline asm
	mov.u32 %r342, 0x0;
	@%p61 atom.global.gpu.acq_rel.add.f32 %r342, [ %rd30 + 0 ], %r343;
	// end inline asm
	mov.b32 	%r345, %f279;
	// begin inline asm
	mov.u32 %r344, 0x0;
	@%p61 atom.global.gpu.acq_rel.add.f32 %r344, [ %rd31 + 0 ], %r345;
	// end inline asm
	mov.b32 	%r347, %f280;
	// begin inline asm
	mov.u32 %r346, 0x0;
	@%p61 atom.global.gpu.acq_rel.add.f32 %r346, [ %rd32 + 0 ], %r347;
	// end inline asm
	mov.b32 	%r349, %f281;
	// begin inline asm
	mov.u32 %r348, 0x0;
	@%p65 atom.global.gpu.acq_rel.add.f32 %r348, [ %rd33 + 0 ], %r349;
	// end inline asm
	mov.b32 	%r351, %f282;
	// begin inline asm
	mov.u32 %r350, 0x0;
	@%p65 atom.global.gpu.acq_rel.add.f32 %r350, [ %rd34 + 0 ], %r351;
	// end inline asm
	mov.b32 	%r353, %f283;
	// begin inline asm
	mov.u32 %r352, 0x0;
	@%p65 atom.global.gpu.acq_rel.add.f32 %r352, [ %rd35 + 0 ], %r353;
	// end inline asm
	mov.b32 	%r355, %f284;
	// begin inline asm
	mov.u32 %r354, 0x0;
	@%p65 atom.global.gpu.acq_rel.add.f32 %r354, [ %rd36 + 0 ], %r355;
	// end inline asm
	mov.b32 	%r357, %f285;
	// begin inline asm
	mov.u32 %r356, 0x0;
	@%p69 atom.global.gpu.acq_rel.add.f32 %r356, [ %rd37 + 0 ], %r357;
	// end inline asm
	mov.b32 	%r359, %f286;
	// begin inline asm
	mov.u32 %r358, 0x0;
	@%p69 atom.global.gpu.acq_rel.add.f32 %r358, [ %rd38 + 0 ], %r359;
	// end inline asm
	mov.b32 	%r361, %f287;
	// begin inline asm
	mov.u32 %r360, 0x0;
	@%p69 atom.global.gpu.acq_rel.add.f32 %r360, [ %rd39 + 0 ], %r361;
	// end inline asm
	mov.b32 	%r363, %f288;
	// begin inline asm
	mov.u32 %r362, 0x0;
	@%p69 atom.global.gpu.acq_rel.add.f32 %r362, [ %rd40 + 0 ], %r363;
	// end inline asm
	.loc	1 66 4
	ret;
$L__tmp1:
$L__func_end0:

}
	.file	1 "/data/sunyunbo/workspace/src/ai_practice/triton/main.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 93
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 109
.b8 97
.b8 105
.b8 110
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 100
.b8 97
.b8 116
.b8 97
.b8 47
.b8 115
.b8 117
.b8 110
.b8 121
.b8 117
.b8 110
.b8 98
.b8 111
.b8 47
.b8 119
.b8 111
.b8 114
.b8 107
.b8 115
.b8 112
.b8 97
.b8 99
.b8 101
.b8 47
.b8 115
.b8 114
.b8 99
.b8 47
.b8 97
.b8 105
.b8 95
.b8 112
.b8 114
.b8 97
.b8 99
.b8 116
.b8 105
.b8 99
.b8 101
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
